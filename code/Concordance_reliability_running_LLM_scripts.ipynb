{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6b1a92",
   "metadata": {},
   "source": [
    "# Concordance & reliability (temp = 0.1, n= 200) + code to run all abstracts\n",
    "\n",
    "This notebook runs **three repeats** of the `infer` pipeline at temperature **0.1** on the first **200** abstracts,\n",
    "then compares the runs pairwise to check self-consistency.\n",
    "\n",
    "**Assumptions**\n",
    "- Your working directory contains `5_concordance.py` and `human_review.xlsx`.\n",
    "- Your Excel sheet with inputs is named `in` (change `SHEET` below if needed).\n",
    "- Caching is disabled to force fresh API calls.\n",
    "\n",
    "It also estimates concordance (human vs human, human vs LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "593c441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEL=human_review.xlsx SHEET=in TEMP=0.1 LIMIT=200 RUN_PREFIX=temp0.1_run\n",
      "Using: /Users/wangmengyao/Desktop/policyclaims/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "EXCEL = \"human_review.xlsx\"\n",
    "SHEET = \"in\"             \n",
    "TEMP = 0.1\n",
    "LIMIT = 200\n",
    "RUN_PREFIX = \"temp0.1_run\"  # run names will be temp0.1_run1, temp0.1_run2, temp0.1_run3\n",
    "\n",
    "print(f\"EXCEL={EXCEL} SHEET={SHEET} TEMP={TEMP} LIMIT={LIMIT} RUN_PREFIX={RUN_PREFIX}\")\n",
    "\n",
    "import sys\n",
    "PYTHON = sys.executable\n",
    "print(\"Using:\", PYTHON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ce670",
   "metadata": {},
   "source": [
    "## 1) Run inference (three repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27a7f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 cached results, processing 200 new items\n",
      "Processing: 100%|██████████| 200/200 [01:14<00:00,  2.68it/s, Saved 200 results]\n",
      "[OK] Saved: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv\n",
      "Total processed: 200 (0 from cache, 200 new)\n"
     ]
    }
   ],
   "source": [
    "!$PYTHON 5_concordance.py infer \\\n",
    "  --excel \"$EXCEL\" \\\n",
    "  --sheet \"$SHEET\" \\\n",
    "  --prompt-name \"${RUN_PREFIX}1\" \\\n",
    "  --temperature \"$TEMP\" \\\n",
    "  --limit \"$LIMIT\" \\\n",
    "  --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7023da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 cached results, processing 200 new items\n",
      "Processing: 100%|██████████| 200/200 [01:09<00:00,  2.86it/s, Saved 200 results]\n",
      "[OK] Saved: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_outputs/run_.1_run2_temp0.1_2025-11-02_165445.csv\n",
      "Total processed: 200 (0 from cache, 200 new)\n"
     ]
    }
   ],
   "source": [
    "# Repeat 2\n",
    "!$PYTHON 5_concordance.py infer \\\n",
    "  --excel \"$EXCEL\" \\\n",
    "  --sheet \"$SHEET\" \\\n",
    "  --prompt-name \"${RUN_PREFIX}2\" \\\n",
    "  --temperature \"$TEMP\" \\\n",
    "  --limit \"$LIMIT\" \\\n",
    "  --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34cb8a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 cached results, processing 200 new items\n",
      "Processing: 100%|██████████| 200/200 [01:13<00:00,  2.74it/s, Saved 200 results]\n",
      "[OK] Saved: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_outputs/run_.1_run3_temp0.1_2025-11-02_165602.csv\n",
      "Total processed: 200 (0 from cache, 200 new)\n"
     ]
    }
   ],
   "source": [
    "# Repeat 3\n",
    "!$PYTHON 5_concordance.py infer \\\n",
    "  --excel \"$EXCEL\" \\\n",
    "  --sheet \"$SHEET\" \\\n",
    "  --prompt-name \"${RUN_PREFIX}3\" \\\n",
    "  --temperature \"$TEMP\" \\\n",
    "  --limit \"$LIMIT\" \\\n",
    "  --no-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc7c6d",
   "metadata": {},
   "source": [
    "## 2) Pairwise comparisons (run to run reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df49a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Finding all run files ---\n",
      "Found Run 1: ../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv\n",
      "Found Run 2: ../concordance/concordance_outputs/run_.1_run2_temp0.1_2025-11-02_165445.csv\n",
      "Found Run 3: ../concordance/concordance_outputs/run_.1_run3_temp0.1_2025-11-02_165602.csv\n",
      "\n",
      "--- Step 2: Running all pairwise comparisons ---\n",
      "\n",
      "----- Comparing Run 1 vs Run 2 -----\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_.1_run2_temp0.1_2025-11-02_165445.csv`\n",
      "\n",
      "**Percent agreement:** 0.990\n",
      "**Cohen's kappa:** 0.973\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[149, 0]\n",
      "[2, 49]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_165608.md\n",
      "\n",
      "----- Comparing Run 1 vs Run 3 -----\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_.1_run3_temp0.1_2025-11-02_165602.csv`\n",
      "\n",
      "**Percent agreement:** 0.990\n",
      "**Cohen's kappa:** 0.973\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[149, 0]\n",
      "[2, 49]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_165609.md\n",
      "\n",
      "----- Comparing Run 2 vs Run 3 -----\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run2_temp0.1_2025-11-02_165445.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_.1_run3_temp0.1_2025-11-02_165602.csv`\n",
      "\n",
      "**Percent agreement:** 1.000\n",
      "**Cohen's kappa:** 1.000\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[151, 0]\n",
      "[0, 49]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_165610.md\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "print(\"--- Step 1: Finding all run files ---\")\n",
    "run_files = {}\n",
    "all_files_found = True\n",
    "\n",
    "# Find the output CSV for each of the three runs\n",
    "for i in [1, 2, 3]:\n",
    "    pattern = f\"../concordance/concordance_outputs/run_*{i}_temp{TEMP}_*.csv\"\n",
    "    found_files = glob.glob(pattern)\n",
    "    \n",
    "    if len(found_files) == 1:\n",
    "        run_files[i] = found_files[0]\n",
    "        print(f\"Found Run {i}: {run_files[i]}\")\n",
    "    else:\n",
    "        print(f\"Error: Expected 1 file for pattern '{pattern}', but found {len(found_files)}.\")\n",
    "        all_files_found = False\n",
    "\n",
    "# Proceed only if all three files were located successfully\n",
    "if all_files_found:\n",
    "    print(\"\\n--- Step 2: Running all pairwise comparisons ---\")\n",
    "    \n",
    "    # Automatically generate pairs: (1, 2), (1, 3), (2, 3)\n",
    "    run_numbers = sorted(run_files.keys())\n",
    "    for i, j in itertools.combinations(run_numbers, 2):\n",
    "        file_a = run_files[i]\n",
    "        file_b = run_files[j]\n",
    "        \n",
    "        print(f\"\\n----- Comparing Run {i} vs Run {j} -----\")\n",
    "        \n",
    "        # Run the compare command with the specific files found\n",
    "        !$PYTHON 5_concordance.py compare --run-a {file_a} --run-b {file_b}\n",
    "else:\n",
    "    print(\"\\nSkipping comparisons because one or more run files could not be found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inter_rater_reliability_header",
   "metadata": {},
   "source": [
    "## 3) Concordance (Human vs. Human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "inter_rater_reliability_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Human Rater DB vs. EC ---\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_from_DBreview_2025-09-02_213212.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_from_ECreview_2025-09-02_213357.csv`\n",
      "\n",
      "**Percent agreement:** 0.949\n",
      "**Cohen's kappa:** 0.834\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[77, 4]\n",
      "[1, 16]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_165612.md\n"
     ]
    }
   ],
   "source": [
    "# This cell assumes the 'db_review' and 'ec_review' variables were defined in the cell above.\n",
    "# Define the files generated from human reviews\n",
    "db_review = '../concordance/concordance_outputs/run_from_DBreview_2025-09-02_213212.csv'\n",
    "ec_review = '../concordance/concordance_outputs/run_from_ECreview_2025-09-02_213357.csv'\n",
    "mw_review = '../concordance/concordance_outputs/run_from_MWreview_2025-09-02_213416.csv'\n",
    "print(f\"\\n--- Comparing Human Rater DB vs. EC ---\")\n",
    "!$PYTHON 5_concordance.py compare \\\n",
    "  --run-a {db_review} \\\n",
    "  --run-b {ec_review}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf27ea",
   "metadata": {},
   "source": [
    "## 4) Concordance LLM vs human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75381dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing LLM Run (run_.1_run1_temp0.1_2025-11-02_165332.csv) vs. DB Review ---\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_from_DBreview_2025-09-02_213212.csv`\n",
      "\n",
      "**Percent agreement:** 0.908\n",
      "**Cohen's kappa:** 0.725\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[73, 1]\n",
      "[8, 16]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_170552.md\n",
      "\n",
      "--- Comparing LLM Run (run_.1_run1_temp0.1_2025-11-02_165332.csv) vs. EC Review ---\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_from_ECreview_2025-09-02_213357.csv`\n",
      "\n",
      "**Percent agreement:** 0.882\n",
      "**Cohen's kappa:** 0.707\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[73, 3]\n",
      "[10, 24]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_170554.md\n",
      "\n",
      "--- Comparing LLM Run (run_.1_run1_temp0.1_2025-11-02_165332.csv) vs. MW Review ---\n",
      "# A/B Concordance\n",
      "- Run A: `../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv`\n",
      "- Run B: `../concordance/concordance_outputs/run_from_MWreview_2025-09-02_213416.csv`\n",
      "\n",
      "**Percent agreement:** 0.939\n",
      "**Cohen's kappa:** 0.843\n",
      "\n",
      "## Confusion matrix\n",
      "Labels: ['NO', 'YES']\n",
      "```\n",
      "[69, 4]\n",
      "[2, 23]\n",
      "```\n",
      "\n",
      "[OK] Saved A/B report: /Users/wangmengyao/Desktop/policyclaims/concordance/concordance_reports/ab_compare_2025-11-02_170555.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# CORRECTED: Use the file from Run 1 generated in this notebook\n",
    "llm_run = run_files[1] \n",
    "\n",
    "print(f\"--- Comparing LLM Run ({os.path.basename(llm_run)}) vs. DB Review ---\")\n",
    "!$PYTHON 5_concordance.py compare \\\n",
    "  --run-a {llm_run} \\\n",
    "  --run-b {db_review}\n",
    "\n",
    "print(f\"\\n--- Comparing LLM Run ({os.path.basename(llm_run)}) vs. EC Review ---\")\n",
    "!$PYTHON 5_concordance.py compare \\\n",
    "  --run-a {llm_run} \\\n",
    "  --run-b {ec_review}\n",
    "\n",
    "print(f\"\\n--- Comparing LLM Run ({os.path.basename(llm_run)}) vs. MW Review ---\")\n",
    "!$PYTHON 5_concordance.py compare \\\n",
    "  --run-a {llm_run} \\\n",
    "  --run-b {mw_review}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46df58e",
   "metadata": {},
   "source": [
    "## 5) Run on all abstracts\n",
    "\n",
    "Recommendation: Preferably, copy the command below into a terminal. This is safer for a long-running process than executing it within a Jupyter Notebook.\n",
    "\n",
    "Estimated Time (40k abstracts):  8-10 hours, $3-5 (assuming discounted deepseek rates)\n",
    "\n",
    "How to Resume: If the process is interrupted, it will resume automatically. To resume, simply run the exact same command again.\n",
    "\n",
    "Note caffeinate command to prevent screen sleeping in macos\n",
    "\n",
    "### Outputs \n",
    "A JSON file (e.g., all_abstracts_LLM.json): This is the main output file, containing the original data plus the new llm_policy_claim classification for each abstract.\n",
    "\n",
    "A CSV file (e.g., all_abstracts_LLM.csv): A CSV version of the same results for easier use in other programs.\n",
    "\n",
    "A \"missed\" CSV file (e.g., data/all_abstracts_LLM_missed.csv): This file will contain only the abstracts that failed to be classified after all API retries, which is useful for debugging.\n",
    "\n",
    "A log file (e.g., data/all_abstracts_LLM_missed.log): This tracks the progress of the run, noting how many abstracts are classified at different stages (checkpoints and final)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8dfe07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wangmengyao/Desktop/policyclaims/.venv/bin/python: can't open file '/Users/wangmengyao/Desktop/policyclaims/code/code/3_llm process_API.py': [Errno 2] No such file or directory\n",
      "/Users/wangmengyao/Desktop/policyclaims/.venv/bin/python3: can't open file '/Users/wangmengyao/Desktop/policyclaims/code/code/3_llmprocess_API.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python \"code/3_llm process_API.py\" \"data/json_files/filtered/all_abstracts.json\"\n",
    "\n",
    "# using  caffeinate command to prevent macos from sleeping and stopping API calls...\n",
    "\n",
    "!caffeinate python3 \"code/3_llmprocess_API.py\" \"data/json_files/filtered/all_abstracts.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4236794",
   "metadata": {},
   "source": [
    "✅ Classification Complete!\n",
    "\n",
    "Total abstracts in output file: 45808\n",
    "Successfully classified: 45807 (11735 True)\n",
    "Failed/unclassified: 1\n",
    "Total script execution time: 35445.78 seconds\n",
    "\n",
    "\n",
    "[thus 9.8 hours, approx $3 in old API fees ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5cc67",
   "metadata": {},
   "source": [
    "### Validate findings from main with a n=200 validation set\n",
    "\n",
    "e.g., compare outputs/run_.1_run1_temp0.1_2025-09-03_095832.csv with json_files/filtered/all_abstracts_LLM.csv\n",
    "\n",
    "should expect same run to run reliability as prior checks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6dbe772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set columns: ['id', 'scopus_id', 'doi', 'title', 'prompt_name', 'prompt_hash', 'model', 'temperature', 'llm_output', 'llm_label']\n",
      "Main file columns: ['scopus_id', 'doi', 'title', 'journal', 'publication_year', 'keywords', 'abstract', 'article_type', 'corresponding_author_country', 'cited_by_count', 'llm_policy_claim']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_file = \"../concordance/concordance_outputs/run_.1_run1_temp0.1_2025-11-02_165332.csv\"\n",
    "val_df = pd.read_csv(val_file)\n",
    "main_file = \"../data/json_files/filtered/all_abstracts_LLM.csv\"\n",
    "main_df = pd.read_csv(main_file)\n",
    "\n",
    "print(\"Validation set columns:\", val_df.columns.tolist())\n",
    "print(\"Main file columns:\", main_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d30cfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing variables:\n",
      "  Validation set: llm_label (converted to boolean as 'policy_claim_val')\n",
      "  Main file: llm_policy_claim (converted to boolean as 'main_claim')\n",
      "Number of matched, non-missing rows: 195\n",
      "Concordance between validation set and main results: 96.4% (188/195)\n",
      "Cohen's kappa: 0.900\n",
      "\n",
      "Sample comparison rows:\n",
      "               scopus_id llm_label llm_policy_claim  policy_claim_val  \\\n",
      "0   scopus_id:0032978484        NO            False             False   \n",
      "1   scopus_id:0032931075        NO            False             False   \n",
      "2  scopus_id:84895890934        NO            False             False   \n",
      "3  scopus_id:78650633722        NO            False             False   \n",
      "4  scopus_id:85066260594       YES            False              True   \n",
      "\n",
      "   main_claim  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "\n",
      "Number of discordant entries: 7\n",
      "Sample discordant entries:\n",
      "            scopus_id llm_label llm_policy_claim  policy_claim_val  main_claim\n",
      "scopus_id:85066260594       YES            False              True       False\n",
      "scopus_id:78650909890       YES            False              True       False\n",
      "scopus_id:84940038714       YES            False              True       False\n",
      "scopus_id:85203007053       YES            False              True       False\n",
      "scopus_id:84996538929       YES            False              True       False\n",
      " scopus_id:0036721712        NO             True             False        True\n",
      "scopus_id:85163105729        NO             True             False        True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_file = \"../concordance/concordance_outputs/run_.1_run3_temp0.1_2025-11-02_165602.csv\"\n",
    "val_df = pd.read_csv(val_file)\n",
    "main_file = \"../data/json_files/filtered/all_abstracts_LLM.csv\"\n",
    "main_df = pd.read_csv(main_file)\n",
    "\n",
    "# Standardize scopus_id for robust matching\n",
    "val_df['scopus_id'] = val_df['scopus_id'].astype(str).str.strip().str.lower()\n",
    "main_df['scopus_id'] = main_df['scopus_id'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Use llm_label as the validation set's policy claim\n",
    "val_df['policy_claim_val'] = val_df['llm_label'].astype(str).str.strip().str.upper() == \"YES\"\n",
    "main_df['main_claim'] = main_df['llm_policy_claim'].astype(bool)\n",
    "\n",
    "# Merge on scopus_id\n",
    "merged = pd.merge(val_df, main_df, on=\"scopus_id\", suffixes=('_val', '_main'))\n",
    "\n",
    "# Only keep rows where both are not missing\n",
    "mask = merged['llm_label'].notna() & merged['llm_policy_claim'].notna()\n",
    "filtered = merged[mask].copy()\n",
    "\n",
    "# Calculate agreement\n",
    "agreement = (filtered[\"policy_claim_val\"] == filtered[\"main_claim\"]).mean()\n",
    "\n",
    "#kappa too\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kappa = cohen_kappa_score(filtered[\"policy_claim_val\"], filtered[\"main_claim\"])\n",
    "\n",
    "print(\"Comparing variables:\")\n",
    "print(\"  Validation set: llm_label (converted to boolean as 'policy_claim_val')\")\n",
    "print(\"  Main file: llm_policy_claim (converted to boolean as 'main_claim')\")\n",
    "print(f\"Number of matched, non-missing rows: {len(filtered)}\")\n",
    "print(f\"Concordance between validation set and main results: {agreement*100:.1f}% ({agreement*len(filtered):.0f}/{len(filtered)})\")\n",
    "print(f\"Cohen's kappa: {kappa:.3f}\")\n",
    "\n",
    "# Show 5 sample rows\n",
    "print(\"\\nSample comparison rows:\")\n",
    "print(filtered[['scopus_id', 'llm_label', 'llm_policy_claim', 'policy_claim_val', 'main_claim']].head(5))\n",
    "\n",
    "# Show up to 10 discordant rows\n",
    "discordant = filtered[filtered[\"policy_claim_val\"] != filtered[\"main_claim\"]]\n",
    "print(f\"\\nNumber of discordant entries: {len(discordant)}\")\n",
    "if len(discordant) > 0:\n",
    "    print(\"Sample discordant entries:\")\n",
    "    print(discordant[['scopus_id', 'llm_label', 'llm_policy_claim', 'policy_claim_val', 'main_claim']].head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
